{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, we will learn how to work with and predict time series. Time series are a collection of **time-dependent data** points. That means that each data point is assigned to a specific timestamp. Ideally, these data points are in chronological order and in contant time intervals (e.g. every minute or everyday). The time series forecasting problem **analyzes patterns in the past data to make predictions about the future**. The most popular example is probably stock price prediction. Other examples are sales of seasonal clothing or weather forecasts. In contrast to regression problems, time series are time-dependent and show specific characteristics, such as **trend and seasonality**.\n",
    "https://www.kaggle.com/code/iamleonie/intro-to-time-series-forecasting\n",
    "\n",
    "**Overview**\n",
    "* [Problem Definition](#Problem-Definition)<br>\n",
    "* [Data Collection](#Data-Collection)<br>\n",
    "* [Data Preprocessing](#Data-Preprocessing)<br>\n",
    "    * [Chronological Order and Equidistant Timestamps](#[Chronological-Order-and-Equidistant-Timestamps])<br>\n",
    "    * [Handling Missing Values](#Handling-Missing-Values)<br>\n",
    "    * [Resampling](#Resampling)<br>\n",
    "    * [Stationarity](#Stationarity)<br>\n",
    "* [Feature Engineering](#Feature-Engineering)<br>\n",
    "    * [Time Features](#Time-Features)<br>\n",
    "    * [Decomposition](#Decomposition)<br>\n",
    "    * [Lag](#Lag)<br>\n",
    "* [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "    * [Autocorrelation Analysis](#Autocorrelation-Analysis)<br> \n",
    "* [Cross Validation](#Cross-Validation)<br>\n",
    "* [Models](#Models)<br>\n",
    "    * [Models for Univariate Time Series](#Models-for-Univariate-Time-Series)<br>\n",
    "        * [Naive Approach](#Naive-Approach)<br>\n",
    "        * [Moving Average](#Moving-Average)<br>\n",
    "        * [Exponential Smoothing  (IN WORK)](#MExponential-Smoothing)<br>\n",
    "        * [ARIMA](#ARIMA)<br>\n",
    "    * [Models for Multivariate Time Series](#Models-for-Multivariate-Time-Series)<br>\n",
    "        * [Vector Autoregression (VAR)](#Vector-Autoregression)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "For this tutorial, we will build a model to predict the depth to groundwater of an aquifer located in Petrignano, Italy. The question we want to answer is\n",
    "> What is the future depth to groundwater of a well belonging to the aquifier in Petrigrano over the next quarter?\n",
    "\n",
    "> The wells field of the alluvial plain between Ospedalicchio di Bastia Umbra and Petrignano is fed by three underground aquifers separated by low permeability septa. The aquifer can be considered a water table groundwater and is also fed by the Chiascio river. The groundwater levels are influenced by the following parameters: rainfall, depth to groundwater, temperatures and drainage volumes, level of the Chiascio river.\n",
    "\n",
    "> Indeed, both rainfall and temperature affect features like level, flow, depth to groundwater and hydrometry some time after it fell down. \n",
    "\n",
    "# Data Collection \n",
    "In a typical workflow for time series, this would be the time for data collection. In this example, we will skip the data collection step and use data from the [Acea Smart Water Analytics challenge](https://www.kaggle.com/c/acea-water-prediction/). Therefore, this section will be a dataset overview. \n",
    "\n",
    "Although the dataset contains multiple waterbodies, we will only be looking at the Aquifer_Petrignano.csv file.\n",
    "\n",
    "Time series data usually comes in **tabular** format (e.g. csv files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /xhome/community-t-lab-io/.local/lib/python3.10/site-packages (0.11.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.8.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn) (1.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn) (9.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.0.0\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.0/307.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn) (1.23.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn) (1.8.1)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=7d8b5aa79bf2cdcdf4b40b4e8b7b046ee19935bda98b95a1b29e4ad1d09843b4\n",
      "  Stored in directory: /xhome/community-t-lab-io/.cache/pip/wheels/9b/13/01/6f3a7fd641f90e1f6c8c7cded057f3394f451f340371c68f3d\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.1.2 sklearn-0.0 threadpoolctl-3.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Depth_to_Groundwater_P24', 'Temperature_Petrignano'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintro-to-time-series-forecasting.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m### Simplifications for the sake of the tutorial ###\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Drop data before 2009 for the purpose of this tutorial\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#df = df[df.Rainfall_Bastia_Umbra.notna()].reset_index(drop=True)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Drop one of the target columns, so we can focus on only one target\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDepth_to_Groundwater_P24\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTemperature_Petrignano\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Simplify column names\u001b[39;00m\n\u001b[1;32m     22\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRainfall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDepth_to_Groundwater\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemperature\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDrainage_Volume\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRiver_Hydrometry\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4806\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   4807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   4808\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4815\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4816\u001b[0m ):\n\u001b[1;32m   4817\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4818\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   4819\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4952\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   4953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4956\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4267\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4311\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4312\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4314\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6644\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6645\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Depth_to_Groundwater_P24', 'Temperature_Petrignano'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import seaborn as sns # Visualization\n",
    "import matplotlib.pyplot as plt # Visualization\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import math\n",
    "\n",
    "import warnings # Supress warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(\"intro-to-time-series-forecasting.csv\")\n",
    "\n",
    "### Simplifications for the sake of the tutorial ###\n",
    "# Drop data before 2009 for the purpose of this tutorial\n",
    "#df = df[df.Rainfall_Bastia_Umbra.notna()].reset_index(drop=True)\n",
    "# Drop one of the target columns, so we can focus on only one target\n",
    "df = df.drop(['Depth_to_Groundwater_P24', 'Temperature_Petrignano'], axis=1)\n",
    "\n",
    "# Simplify column names\n",
    "df.columns = ['Date', 'Rainfall', 'Depth_to_Groundwater', 'Temperature', 'Drainage_Volume', 'River_Hydrometry']\n",
    "\n",
    "targets = ['Depth_to_Groundwater']\n",
    "features = [feature for feature in df.columns if feature not in targets]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with time series, the most essential features are the time related feature. In this example, we have the column `Date` which  uniquely identifies a day. Ideally, the data is already in chronological order and the time stamps are equidistant in time series. This is already the case in our data: The time interval is one day and the data is already in chronological order. Therefore, we do not have to do this additional data preparation step. \n",
    "\n",
    "\n",
    "This column is provided in string format. Let's convert it to the `datetime64[ns]` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date \n",
    "\n",
    "df['Date'] = pd.to_datetime(df.Date, format = '%d/%m/%Y')\n",
    "df.head().style.set_properties(subset=['Date'], **{'background-color': 'dodgerblue'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features:\n",
    "* **Rainfall** indicates the quantity of rain falling (mm)\n",
    "* **Temperature** indicates the temperature (°C) \n",
    "* **Volume** indicates the volume of water taken from the drinking water treatment plant (m$^3$)\n",
    "* **Hydrometry** indicates the groundwater level (m)\n",
    "\n",
    "Target:\n",
    "* **Depth to Groundwater** indicates the groundwater level (m from the ground floor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=5, ncols=1, figsize=(15, 25))\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Rainfall.fillna(np.inf), ax=ax[0], color='dodgerblue')\n",
    "ax[0].set_title('Feature: Rainfall', fontsize=14)\n",
    "ax[0].set_ylabel(ylabel='Rainfall', fontsize=14)\n",
    "\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Temperature.fillna(np.inf), ax=ax[1], color='dodgerblue',label='Bastia Umbra')\n",
    "ax[1].set_title('Feature: Temperature', fontsize=14)\n",
    "ax[1].set_ylabel(ylabel='Temperature', fontsize=14)\n",
    "\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(np.inf), ax=ax[2], color='dodgerblue')\n",
    "ax[2].set_title('Feature: Volume', fontsize=14)\n",
    "ax[2].set_ylabel(ylabel='Volume', fontsize=14)\n",
    "\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.River_Hydrometry.fillna(np.inf), ax=ax[3], color='dodgerblue')\n",
    "ax[3].set_title('Feature: Hydrometry', fontsize=14)\n",
    "ax[3].set_ylabel(ylabel='Hydrometry', fontsize=14)\n",
    "\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Depth_to_Groundwater.fillna(np.inf), ax=ax[4], color='dodgerblue')\n",
    "ax[4].set_title('Target: Depth to Groundwater', fontsize=14)\n",
    "ax[4].set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    ax[i].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Chronological Order and Equidistant Timestamps\n",
    "The data should be in **chronological order** and the **timestamps should be equidistant** in time series. The chronological order can be achieved by sorting the dataframe by the timestamps. Equidisant timestamps indicates constant time intervals. To check this, the difference between each timestamp can be taken. If this is not the case, you can decide on a constant time interval and resample the data (see [Resampling](#Resampling)).\n",
    "\n",
    "This is already the case in our data: The time interval is one day and the data is already in chronological order. Therefore, we do not have to do this additional data preparation step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values by timestamp (not necessary in this case)\n",
    "df = df.sort_values(by='Date')\n",
    "\n",
    "# Check time intervals\n",
    "df['Time_Interval'] = df.Date - df.Date.shift(1)\n",
    "\n",
    "df[['Date', 'Time_Interval']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "print(f\"{df['Time_Interval'].value_counts()}\")\n",
    "df = df.drop('Time_Interval', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "We can see that `Depth_to_Groundwater` has missing values.\n",
    "\n",
    "Furthermore, plotting the time series reveals that there seem to be some **implausible zero values** for `Drainage_Volume`, and `River_Hydrometry`. We will have to clean them by replacing them by `nan` values and filling them afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 15))\n",
    "old = df.River_Hydrometry.copy()\n",
    "df['River_Hydrometry'] = np.where((df.River_Hydrometry == 0),np.nan, df.River_Hydrometry)\n",
    "\n",
    "sns.lineplot(x=df.Date, y=old.fillna(np.inf), ax=ax[0], color='darkorange', label = 'original')\n",
    "sns.lineplot(x=df.Date, y=df.River_Hydrometry.fillna(np.inf), ax=ax[0], color='dodgerblue', label = 'modified')\n",
    "ax[0].set_title('Feature: Hydrometry', fontsize=14)\n",
    "ax[0].set_ylabel(ylabel='Hydrometry', fontsize=14)\n",
    "\n",
    "old = df.Drainage_Volume.copy()\n",
    "df['Drainage_Volume'] = np.where((df.Drainage_Volume == 0),np.nan, df.Drainage_Volume)\n",
    "\n",
    "sns.lineplot(x=df.Date, y=old.fillna(np.inf), ax=ax[1], color='darkorange', label = 'original')\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(np.inf), ax=ax[1], color='dodgerblue', label = 'modified')\n",
    "ax[1].set_title('Feature: Volume', fontsize=14)\n",
    "ax[1].set_ylabel(ylabel='Volume', fontsize=14)\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to think about what to do with these missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,5))\n",
    "sns.heatmap(df.T.isna(), cmap='Blues')\n",
    "ax.set_title('Fields with Missing Values', fontsize=16)\n",
    "#for tick in ax.xaxis.get_major_ticks():\n",
    "#    tick.label.set_fontsize(14) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Option 1: Fill NaN with Outlier or Zero**\n",
    "\n",
    "    In this specific example filling the missing value with an outlier value such as -999 is not a good idea. However, many notebooks in this challenge have been using -999. \n",
    "    \n",
    "* **Option 2: Fill NaN with Mean Value**\n",
    "\n",
    "    Also in this example, we can see that filling NaNs with the mean value is also not sufficient.\n",
    "\n",
    "* **Option 3: Fill NaN with Last Value with `.ffill()`**\n",
    "\n",
    "    Filling NaNs with the last value is already a little bit better in this case.\n",
    "\n",
    "* **Option 4: Fill NaN with Linearly Interpolated Value with `.interpolate()`**\n",
    "\n",
    "    Filling NaNs with the interpolated values is the best option in this small examlple but it requires knowledge of the neighouring values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=4, ncols=1, figsize=(15, 12))\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(0), ax=ax[0], color='darkorange', label = 'modified')\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(np.inf), ax=ax[0], color='dodgerblue', label = 'original')\n",
    "ax[0].set_title('Fill NaN with 0', fontsize=14)\n",
    "ax[0].set_ylabel(ylabel='Volume C10 Petrignano', fontsize=14)\n",
    "\n",
    "mean_val = df.Drainage_Volume.mean()\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(mean_val), ax=ax[1], color='darkorange', label = 'modified')\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(np.inf), ax=ax[1], color='dodgerblue', label = 'original')\n",
    "ax[1].set_title(f'Fill NaN with Mean Value ({mean_val:.0f})', fontsize=14)\n",
    "ax[1].set_ylabel(ylabel='Volume C10 Petrignano', fontsize=14)\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume.ffill(), ax=ax[2], color='darkorange', label = 'modified')\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(np.inf), ax=ax[2], color='dodgerblue', label = 'original')\n",
    "ax[2].set_title(f'FFill', fontsize=14)\n",
    "ax[2].set_ylabel(ylabel='Volume C10 Petrignano', fontsize=14)\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume.interpolate(), ax=ax[3], color='darkorange', label = 'modified')\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(np.inf), ax=ax[3], color='dodgerblue', label = 'original')\n",
    "ax[3].set_title(f'Interpolate', fontsize=14)\n",
    "ax[3].set_ylabel(ylabel='Volume C10 Petrignano', fontsize=14)\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].set_xlim([date(2019, 5, 1), date(2019, 10, 1)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Drainage_Volume'] = df['Drainage_Volume'].interpolate()\n",
    "df['River_Hydrometry'] = df['River_Hydrometry'].interpolate()\n",
    "df['Depth_to_Groundwater'] = df['Depth_to_Groundwater'].interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "Resampling can provide additional information on the data. There are two types of resampling:\n",
    "* **Upsampling** is when the frequency of samples is increased (e.g. days to hours)\n",
    "* **Downsampling** is when the frequency of samples is decreased (e.g. days to weeks)\n",
    "\n",
    "In this example, we will do some downsampling with the `.resample()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=4, sharex=True, figsize=(16,12))\n",
    "\n",
    "ax[0, 0].bar(df.Date, df.Rainfall, width=5, color='dodgerblue')\n",
    "ax[0, 0].set_title('Daily Rainfall (Acc.)', fontsize=14)\n",
    "\n",
    "resampled_df = df[['Date','Rainfall']].resample('7D', on='Date').sum().reset_index(drop=False)\n",
    "ax[1, 0].bar(resampled_df.Date, resampled_df.Rainfall, width=10, color='dodgerblue')\n",
    "ax[1, 0].set_title('Weekly Rainfall (Acc.)', fontsize=14)\n",
    "\n",
    "resampled_df = df[['Date','Rainfall']].resample('M', on='Date').sum().reset_index(drop=False)\n",
    "ax[2, 0].bar(resampled_df.Date, resampled_df.Rainfall, width=15, color='dodgerblue')\n",
    "ax[2, 0].set_title('Monthly Rainfall (Acc.)', fontsize=14)\n",
    "\n",
    "resampled_df = df[['Date','Rainfall']].resample('12M', on='Date').sum().reset_index(drop=False)\n",
    "ax[3, 0].bar(resampled_df.Date, resampled_df.Rainfall, width=20, color='dodgerblue')\n",
    "ax[3, 0].set_title('Annual Rainfall (Acc.)', fontsize=14)\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i, 0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n",
    "\n",
    "sns.lineplot(df.Date, df.Temperature, color='dodgerblue', ax=ax[0, 1])\n",
    "ax[0, 1].set_title('Daily Temperature (Acc.)', fontsize=14)\n",
    "\n",
    "resampled_df = df[['Date','Temperature']].resample('7D', on='Date').mean().reset_index(drop=False)\n",
    "sns.lineplot(resampled_df.Date, resampled_df.Temperature, color='dodgerblue', ax=ax[1, 1])\n",
    "ax[1, 1].set_title('Weekly Temperature (Acc.)', fontsize=14)\n",
    "\n",
    "resampled_df = df[['Date','Temperature']].resample('M', on='Date').mean().reset_index(drop=False)\n",
    "sns.lineplot(resampled_df.Date, resampled_df.Temperature, color='dodgerblue', ax=ax[2, 1])\n",
    "ax[2, 1].set_title('Monthly Temperature (Acc.)', fontsize=14)\n",
    "\n",
    "resampled_df = df[['Date','Temperature']].resample('365D', on='Date').mean().reset_index(drop=False)\n",
    "sns.lineplot(resampled_df.Date, resampled_df.Temperature, color='dodgerblue', ax=ax[3, 1])\n",
    "ax[3, 1].set_title('Annual Temperature (Acc.)', fontsize=14)\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i, 1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n",
    "    ax[i, 1].set_ylim([-5, 35])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, resampling would not be necessary. On the other hand, there is no necessity to look at the daily data. Considering weekly data seems to be sufficient as well. Therefore, we will **downsample the data to a weekly basis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsampled = df[['Date',\n",
    "                     'Depth_to_Groundwater', \n",
    "                     'Temperature',\n",
    "                     'Drainage_Volume', \n",
    "                     'River_Hydrometry'\n",
    "                    ]].resample('7D', on='Date').mean().reset_index(drop=False)\n",
    "\n",
    "df_downsampled['Rainfall'] = df[['Date',\n",
    "                                 'Rainfall'\n",
    "                                ]].resample('7D', on='Date').sum().reset_index(drop=False)[['Rainfall']]\n",
    "\n",
    "df = df_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "\n",
    "Some time-series models, such as such as [ARIMA](#ARIMA), assume that the underlying data is stationary. \n",
    "Stationarity describes that the time-series has\n",
    "* constant mean and mean is not time-dependent \n",
    "* constant variance and variance is not time-dependent \n",
    "* constant covariance and covariance is not time-dependent \n",
    "\n",
    "> If a time series has a specific (stationary) behavior over a given time interval, then it can be assumed that the time series will behave the same at a later time.\n",
    "\n",
    "Time series **with trend and/or seasonality are not stationary**. Trend indicates that the mean is not constant over time and seasonality indicates that the variance is not constant over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "t = np.linspace(0, 19, 20)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, nrows=1, figsize=(20,4))\n",
    "stationary = [5, 4, 5, 6, 5, 4, 5, 6, 5, 4, 5, 6, 5, 4, 5, 6, 5, 4, 5, 6,]\n",
    "sns.lineplot(x=t, y=stationary, ax=ax[0], color='forestgreen')\n",
    "sns.lineplot(x=t, y=5, ax=ax[0], color='grey')\n",
    "sns.lineplot(x=t, y=6, ax=ax[0], color='grey')\n",
    "sns.lineplot(x=t, y=4, ax=ax[0], color='grey')\n",
    "ax[0].lines[2].set_linestyle(\"--\")\n",
    "ax[0].lines[3].set_linestyle(\"--\")\n",
    "ax[0].set_title(f'Stationary \\nconstant mean \\nconstant variance \\nconstant covariance', fontsize=14)\n",
    "\n",
    "nonstationary1 = [ 9, 0, 1, 10, 8, 1, 2, 9, 7, 2, 3, 8, 6, 3, 4, 7, 5, 4, 5, 6]\n",
    "sns.lineplot(x=t, y=nonstationary1, ax=ax[1], color='indianred' )\n",
    "sns.lineplot(x=t, y=5, ax=ax[1], color='grey')\n",
    "sns.lineplot(x=t, y=t*0.25-0.5, ax=ax[1], color='grey')\n",
    "sns.lineplot(x=t, y=t*(-0.25)+11, ax=ax[1], color='grey')\n",
    "ax[1].lines[2].set_linestyle(\"--\")\n",
    "ax[1].lines[3].set_linestyle(\"--\")\n",
    "ax[1].set_title(f'Non Stationary \\nconstant mean \\n non-constant variance\\nnconstant covariance', fontsize=14)\n",
    "\n",
    "nonstationary2 = [0, 2, 1, 3, 2, 4, 3, 5, 4, 6, 5, 7, 6, 8, 7, 9, 8, 10, 9, 11,]\n",
    "sns.lineplot(x=t, y=nonstationary2, ax=ax[2], color='indianred' )\n",
    "sns.lineplot(x=t, y=t*0.5+0.7, ax=ax[2], color='grey')\n",
    "sns.lineplot(x=t, y=t*0.5, ax=ax[2], color='grey')\n",
    "sns.lineplot(x=t, y=t*0.5+1.5, ax=ax[2], color='grey')\n",
    "ax[2].lines[2].set_linestyle(\"--\")\n",
    "ax[2].lines[3].set_linestyle(\"--\")\n",
    "ax[2].set_title(f'Non Stationary \\n non-constant mean\\nconstant variance\\nnconstant covariance', fontsize=14)\n",
    "\n",
    "nonstationary3 = [5, 4.5, 4, 4.5, 5, 5.5, 6, 5.5, 5, 4.5, 4, 5, 6, 5, 4, 6, 4, 6, 4, 6,]\n",
    "sns.lineplot(x=t, y=nonstationary3, ax=ax[3], color='indianred')\n",
    "sns.lineplot(x=t, y=5, ax=ax[3], color='grey')\n",
    "sns.lineplot(x=t, y=6, ax=ax[3], color='grey')\n",
    "sns.lineplot(x=t, y=4, ax=ax[3], color='grey')\n",
    "ax[3].lines[2].set_linestyle(\"--\")\n",
    "ax[3].lines[3].set_linestyle(\"--\")\n",
    "ax[3].set_title(f'Stationary \\nconstant mean \\nconstant variance \\nnon-constant covariance', fontsize=14)\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].set_ylim([-1, 12])\n",
    "    ax[i].set_xlabel('Time', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The check for stationarity can be done via three different approaches:\n",
    "1. **visually**: plot time series and check for trends or seasonality\n",
    "2. **basic statistics**: split time series and compare the mean and variance of each partition\n",
    "3. **statistical test**: Augmented Dickey Fuller test\n",
    "\n",
    "Let's do the **visual check** first. We can see that all features except `Temperature` have non-constant mean and non-constant variance. Therefore, **none of these seem to be stationary**. However, `Temperature` shows strong seasonality (hot in summer, cold in winter) and therefore it is not stationary either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "rolling_window = 52\n",
    "f, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 12))\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Rainfall, ax=ax[0, 0], color='indianred')\n",
    "sns.lineplot(x=df.Date, y=df.Rainfall.rolling(rolling_window).mean(), ax=ax[0, 0], color='black', label='rolling mean')\n",
    "sns.lineplot(x=df.Date, y=df.Rainfall.rolling(rolling_window).std(), ax=ax[0, 0], color='blue', label='rolling std')\n",
    "ax[0, 0].set_title('Rainfall: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\n",
    "ax[0, 0].set_ylabel(ylabel='Rainfall', fontsize=14)\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Temperature, ax=ax[1, 0], color='indianred')\n",
    "sns.lineplot(x=df.Date, y=df.Temperature.rolling(rolling_window).mean(), ax=ax[1, 0], color='black', label='rolling mean')\n",
    "sns.lineplot(x=df.Date, y=df.Temperature.rolling(rolling_window).std(), ax=ax[1, 0], color='blue', label='rolling std')\n",
    "ax[1, 0].set_title('Temperature: Non-stationary \\nvariance is time-dependent (seasonality)', fontsize=14)\n",
    "ax[1, 0].set_ylabel(ylabel='Temperature', fontsize=14)\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.River_Hydrometry, ax=ax[0, 1], color='indianred')\n",
    "sns.lineplot(x=df.Date, y=df.River_Hydrometry.rolling(rolling_window).mean(), ax=ax[0, 1], color='black', label='rolling mean')\n",
    "sns.lineplot(x=df.Date, y=df.River_Hydrometry.rolling(rolling_window).std(), ax=ax[0, 1], color='blue', label='rolling std')\n",
    "ax[0, 1].set_title('Hydrometry: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\n",
    "ax[0, 1].set_ylabel(ylabel='Hydrometry', fontsize=14)\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume, ax=ax[1, 1], color='indianred')\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume.rolling(rolling_window).mean(), ax=ax[1, 1], color='black', label='rolling mean')\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume.rolling(rolling_window).std(), ax=ax[1, 1], color='blue', label='rolling std')\n",
    "ax[1, 1].set_title('Volume: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\n",
    "ax[1, 1].set_ylabel(ylabel='Volume', fontsize=14)\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Depth_to_Groundwater, ax=ax[2, 0], color='indianred')\n",
    "sns.lineplot(x=df.Date, y=df.Depth_to_Groundwater.rolling(rolling_window).mean(), ax=ax[2, 0], color='black', label='rolling mean')\n",
    "sns.lineplot(x=df.Date, y=df.Depth_to_Groundwater.rolling(rolling_window).std(), ax=ax[2, 0], color='blue', label='rolling std')\n",
    "ax[2, 0].set_title('Depth to Groundwater: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\n",
    "ax[2, 0].set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i,0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n",
    "    ax[i,1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n",
    "\n",
    "f.delaxes(ax[2, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will **check the underlying statistics**. For this we will **split the time series into two sections** and check the mean and the variance. You could do more partitions if you wanted.\n",
    "\n",
    "With this method, `Temperature` and `River_Hydrometry` show **somewhat similar (constant) mean and variance** and could be seen as stationary. However, with this method, we are not able to see the seasonality in the `Temperature` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 2\n",
    "partition_length = int(len(df) / num_partitions)\n",
    "\n",
    "partition1_mean = df.head(partition_length).mean()\n",
    "partition1_var = df.head(partition_length).var()\n",
    "partition2_mean = df.tail(partition_length).mean()\n",
    "partition2_var = df.tail(partition_length).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "stationarity_test = pd.concat([partition1_mean, partition2_mean, partition1_var, partition2_var], axis=1)\n",
    "stationarity_test.columns = ['Partition 1 Mean', 'Partition 2 Mean', 'Partition 1 Variance', 'Partition 2 Variance']\n",
    "\n",
    "def highlight_greater(x):\n",
    "    temp = x.copy()\n",
    "    temp = temp.round(0).astype(int)\n",
    "    m1 = (temp['Partition 1 Mean'] == temp['Partition 2 Mean'])\n",
    "    m2 = (temp['Partition 1 Variance'] == temp['Partition 2 Variance'])\n",
    "    m3 = (temp['Partition 1 Mean'] < temp['Partition 2 Mean']+3) & (temp['Partition 1 Mean'] > temp['Partition 2 Mean']-3)\n",
    "    m4 = (temp['Partition 1 Variance'] < temp['Partition 2 Variance']+3) & (temp['Partition 1 Variance'] > temp['Partition 2 Variance']-3)\n",
    "\n",
    "    df1 = pd.DataFrame('background-color: ', index=x.index, columns=x.columns)\n",
    "    #rewrite values by boolean masks\n",
    "    df1['Partition 1 Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'), df1['Partition 1 Mean'])\n",
    "    df1['Partition 2 Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'), df1['Partition 2 Mean'])\n",
    "    df1['Partition 1 Mean'] = np.where(m3, 'background-color: {}'.format('gold'), df1['Partition 1 Mean'])\n",
    "    df1['Partition 2 Mean'] = np.where(m3, 'background-color: {}'.format('gold'), df1['Partition 2 Mean'])\n",
    "    df1['Partition 1 Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition 1 Mean'])\n",
    "    df1['Partition 2 Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition 2 Mean'])\n",
    "\n",
    "    df1['Partition 1 Variance'] = np.where(~m2, 'background-color: {}'.format('salmon'), df1['Partition 1 Variance'])\n",
    "    df1['Partition 2 Variance'] = np.where(~m2, 'background-color: {}'.format('salmon'), df1['Partition 2 Variance'])\n",
    "    df1['Partition 1 Variance'] = np.where(m4, 'background-color: {}'.format('gold'), df1['Partition 1 Variance'])\n",
    "    df1['Partition 2 Variance'] = np.where(m4, 'background-color: {}'.format('gold'), df1['Partition 2 Variance'])\n",
    "    df1['Partition 1 Variance'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition 1 Variance'])\n",
    "    df1['Partition 2 Variance'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition 2 Variance'])\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "stationarity_test.style.apply(highlight_greater, axis=None).format(\"{:20,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the histograms. Since we are looking at the mean and variance, we are expecting that the data conforms to a Gaussian distribution (bell shaped distribution) in case of stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 9))\n",
    "\n",
    "sns.distplot(df.Rainfall.fillna(np.inf), ax=ax[0, 0], color='indianred')\n",
    "ax[0, 0].set_title('Rainfall: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\n",
    "ax[0, 0].set_ylabel(ylabel='Rainfall', fontsize=14)\n",
    "\n",
    "sns.distplot(df.Temperature.fillna(np.inf), ax=ax[1, 0], color='indianred')\n",
    "ax[1, 0].set_title('Temperature: Non-stationary \\nvariance is time-dependent (seasonality)', fontsize=14)\n",
    "ax[1, 0].set_ylabel(ylabel='Temperature', fontsize=14)\n",
    "\n",
    "sns.distplot(df.River_Hydrometry.fillna(np.inf), ax=ax[0, 1], color='indianred')\n",
    "ax[0, 1].set_title('Hydrometry: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\n",
    "ax[0, 1].set_ylabel(ylabel='Hydrometry', fontsize=14)\n",
    "\n",
    "sns.distplot(df.Drainage_Volume.fillna(np.inf), ax=ax[1, 1], color='indianred')\n",
    "ax[1, 1].set_title('Volume: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\n",
    "ax[1, 1].set_ylabel(ylabel='Volume', fontsize=14)\n",
    "\n",
    "sns.distplot(df.Depth_to_Groundwater.fillna(np.inf), ax=ax[2, 0], color='indianred')\n",
    "ax[2, 0].set_title('Depth to Groundwater: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\n",
    "ax[2, 0].set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n",
    "\n",
    "f.delaxes(ax[2, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Augmented Dickey-Fuller (ADF) test**  is a type of statistical test called a unit root test.  Unit roots are a cause for non-stationarity.\n",
    "\n",
    "* **Null Hypothesis (H0)**: Time series has a unit root. (Time series is **not stationary**).\n",
    "\n",
    "* **Alternate Hypothesis (H1)**: Time series has no unit root (Time series is **stationary**).\n",
    "\n",
    "If the **null hypothesis can be rejected**, we can conclude that the **time series is stationary**.\n",
    "\n",
    "There are two ways to rejects the null hypothesis:\n",
    "\n",
    "On the one hand, the null hypothesis can be rejected if the p-value is below a set significance level. The defaults significance level is 5%\n",
    "\n",
    "* <font color='red'>**p-value > significance level (default: 0.05)**</font>: Fail to reject the null hypothesis (H0), the data has a unit root and is <font color='red'>non-stationary</font>.\n",
    "* <font color='green'>**p-value <= significance level (default: 0.05)**</font>: Reject the null hypothesis (H0), the data does not have a unit root and is <font color='green'>stationary</font>.\n",
    "    \n",
    "On the other hand, the null hypothesis can be rejects if the test statistic is less than the critical value.\n",
    "* <font color='red'>**ADF statistic > critical value**</font>: Fail to reject the null hypothesis (H0), the data has a unit root and is <font color='red'>non-stationary</font>.\n",
    "* <font color='green'>**ADF statistic < critical value**</font>: Reject the null hypothesis (H0), the data does not have a unit root and is <font color='green'>stationary</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "result = adfuller(df.Depth_to_Groundwater.values)\n",
    "adf_stat = result[0]\n",
    "p_val = result[1]\n",
    "crit_val_1 = result[4]['1%']\n",
    "crit_val_5 = result[4]['5%']\n",
    "crit_val_10 = result[4]['10%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "\n",
    "f, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 9))\n",
    "\n",
    "def visualize_adfuller_results(series, title, ax):\n",
    "    result = adfuller(series)\n",
    "    significance_level = 0.05\n",
    "    adf_stat = result[0]\n",
    "    p_val = result[1]\n",
    "    crit_val_1 = result[4]['1%']\n",
    "    crit_val_5 = result[4]['5%']\n",
    "    crit_val_10 = result[4]['10%']\n",
    "\n",
    "    if (p_val < significance_level) & ((adf_stat < crit_val_1)):\n",
    "        linecolor = 'forestgreen' \n",
    "    elif (p_val < significance_level) & (adf_stat < crit_val_5):\n",
    "        linecolor = 'gold'\n",
    "    elif (p_val < significance_level) & (adf_stat < crit_val_10):\n",
    "        linecolor = 'orange'\n",
    "    else:\n",
    "        linecolor = 'indianred'\n",
    "    sns.lineplot(x=df.Date, y=series, ax=ax, color=linecolor)\n",
    "    ax.set_title(f'ADF Statistic {adf_stat:0.3f}, p-value: {p_val:0.3f}\\nCritical Values 1%: {crit_val_1:0.3f}, 5%: {crit_val_5:0.3f}, 10%: {crit_val_10:0.3f}', fontsize=14)\n",
    "    ax.set_ylabel(ylabel=title, fontsize=14)\n",
    "\n",
    "visualize_adfuller_results(df.Rainfall.values, 'Rainfall', ax[0, 0])\n",
    "visualize_adfuller_results(df.Temperature.values, 'Temperature', ax[1, 0])\n",
    "visualize_adfuller_results(df.River_Hydrometry.values, 'River_Hydrometry', ax[0, 1])\n",
    "visualize_adfuller_results(df.Drainage_Volume.values, 'Drainage_Volume', ax[1, 1])\n",
    "visualize_adfuller_results(df.Depth_to_Groundwater.values, 'Depth_to_Groundwater', ax[2, 0])\n",
    "\n",
    "f.delaxes(ax[2, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is not stationary but we want to use a model that requires with characteristic, the data has to be transformed. However, if the data is not stationary to begin with, we should rethink the choice of model.\n",
    "\n",
    "The two most common methods to achieve stationarity are:\n",
    "* **Transformation**: e.g. log or square root to stabilize non-constant variance\n",
    "* **Differencing**: subtracts the current value from the previous \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transform of absolute values\n",
    "# (Log transoform of negative values will return NaN)\n",
    "df['Depth_to_Groundwater_log'] = np.log(abs(df.Depth_to_Groundwater))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 6))\n",
    "visualize_adfuller_results(abs(df.Depth_to_Groundwater), 'Absolute \\n Depth to Groundwater', ax[0, 0])\n",
    "\n",
    "sns.distplot(df.Depth_to_Groundwater_log, ax=ax[0, 1])\n",
    "visualize_adfuller_results(df.Depth_to_Groundwater_log, 'Transformed \\n Depth to Groundwater', ax[1, 0])\n",
    "\n",
    "sns.distplot(df.Depth_to_Groundwater_log, ax=ax[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differencing can be done in different orders:\n",
    "* First order differencing: linear trends with $z_i = y_i - y_{i-1}$\n",
    "* Second-order differencing: quadratic trends with $z_i = (y_i - y_{i-1}) - (y_{i-1} - y_{i-2})$\n",
    "* and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Order Differencing\n",
    "ts_diff = np.diff(df.Depth_to_Groundwater)\n",
    "df['Depth_to_Groundwater_diff_1'] = np.append([0], ts_diff)\n",
    "\n",
    "# Second Order Differencing\n",
    "ts_diff = np.diff(df.Depth_to_Groundwater_diff_1)\n",
    "df['Depth_to_Groundwater_diff_2'] = np.append([0], ts_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 6))\n",
    "\n",
    "visualize_adfuller_results(df.Depth_to_Groundwater_diff_1, 'Differenced (1. Order) \\n Depth to Groundwater', ax[0])\n",
    "visualize_adfuller_results(df.Depth_to_Groundwater_diff_2, 'Differenced (2. Order) \\n Depth to Groundwater', ax[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differencing can be reverted if the the first value before differencing is known. In this case, we can accumulate all values with the function `.cumsum()` and add the first value of the original time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Depth_to_Groundwater.equals(df.Depth_to_Groundwater_diff_1.cumsum() + df.Depth_to_Groundwater.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = pd.DatetimeIndex(df['Date']).year\n",
    "df['month'] = pd.DatetimeIndex(df['Date']).month\n",
    "df['day'] = pd.DatetimeIndex(df['Date']).day\n",
    "df['day_of_year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
    "df['week_of_year'] = pd.DatetimeIndex(df['Date']).weekofyear\n",
    "df['quarter'] = pd.DatetimeIndex(df['Date']).quarter\n",
    "df['season'] = df.month%12 // 3 + 1\n",
    "\n",
    "df[['Date', 'year', 'month', 'day', 'day_of_year', 'week_of_year', 'quarter', 'season']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Cyclical Features \n",
    "The new time features are cyclical. For example,the feature `month` cycles between 1 and 12 for every year.\n",
    "While the difference between each month increments by 1 during the year, between two years the `month` feature jumps from 12 (December) to 1 (January). This results in a -11 difference, which can confuse a lot of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 3))\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.month, color='dodgerblue')\n",
    "ax.set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we want the underlying data to represent the same difference between two consecutive months, even between December and January. A common remedy for this issue is to encode cyclical features into two dimensions with sine and cosine transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "month_in_year = 12\n",
    "df['month_sin'] = np.sin(2*np.pi*df.month/month_in_year)\n",
    "df['month_cos'] = np.cos(2*np.pi*df.month/month_in_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "days_in_month = 30\n",
    "df['day_sin'] = np.sin(2*np.pi*df.day/days_in_month)\n",
    "df['day_cos'] = np.cos(2*np.pi*df.day/days_in_month)\n",
    "\n",
    "days_in_year = 365\n",
    "df['day_of_year_sin'] = np.sin(2*np.pi*df.day_of_year/days_in_year)\n",
    "df['day_of_year_cos'] = np.cos(2*np.pi*df.day_of_year/days_in_year)\n",
    "\n",
    "weeks_in_year = 52.1429\n",
    "df['week_of_year_sin'] = np.sin(2*np.pi*df.week_of_year/weeks_in_year)\n",
    "df['week_of_year_cos'] = np.cos(2*np.pi*df.week_of_year/weeks_in_year)\n",
    "\n",
    "quarters_in_year = 4\n",
    "df['quarter_sin'] = np.sin(2*np.pi*df.quarter/quarters_in_year)\n",
    "df['quarter_cos'] = np.cos(2*np.pi*df.quarter/quarters_in_year)\n",
    "\n",
    "seasons_in_year = 4\n",
    "df['season_sin'] = np.sin(2*np.pi*df.season/seasons_in_year)\n",
    "df['season_cos'] = np.cos(2*np.pi*df.season/seasons_in_year)\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n",
    "\n",
    "sns.scatterplot(x=df.month_sin, y=df.month_cos, color='dodgerblue')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition\n",
    "\n",
    "The **characteristics of a time series** are\n",
    "* Trend and Level\n",
    "* Seasonality\n",
    "* Random / Noise\n",
    "\n",
    "We can use the function `seasonal_decompose()` from the [statsmodels](https://www.statsmodels.org) library.\n",
    "\n",
    "* Additive: $y(t) = Level + Trend + Seasonality + Noise$\n",
    "* Multiplicative: $y(t) = Level * Trend * Seasonality * Noise$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decompose_cols =  ['Rainfall', 'Temperature', \n",
    "                   'Drainage_Volume', \n",
    "                   'River_Hydrometry', 'Depth_to_Groundwater']\n",
    "\n",
    "for col in decompose_cols:\n",
    "    decomp = seasonal_decompose(df[col], freq=52, model='additive', extrapolate_trend='freq')\n",
    "    df[f\"{col}_trend\"] = decomp.trend\n",
    "    df[f\"{col}_seasonal\"] = decomp.seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=4, sharex=True, figsize=(16,8))\n",
    "res = seasonal_decompose(df.Temperature, freq=52, model='additive', extrapolate_trend='freq')\n",
    "\n",
    "ax[0,0].set_title('Decomposition of Temperature', fontsize=16)\n",
    "res.observed.plot(ax=ax[0,0], legend=False, color='dodgerblue')\n",
    "ax[0,0].set_ylabel('Observed', fontsize=14)\n",
    "ax[0,0].set_ylim([-5, 35])\n",
    "\n",
    "res.trend.plot(ax=ax[1,0], legend=False, color='dodgerblue')\n",
    "ax[1,0].set_ylabel('Trend', fontsize=14)\n",
    "ax[1,0].set_ylim([-5, 35])\n",
    "\n",
    "res.seasonal.plot(ax=ax[2,0], legend=False, color='dodgerblue')\n",
    "ax[2,0].set_ylabel('Seasonal', fontsize=14)\n",
    "ax[2,0].set_ylim([-15, 15])\n",
    "\n",
    "res.resid.plot(ax=ax[3,0], legend=False, color='dodgerblue')\n",
    "ax[3,0].set_ylabel('Residual', fontsize=14)\n",
    "ax[3,0].set_ylim([-15, 15])\n",
    "\n",
    "ax[0,1].set_title('Decomposition of Depth_to_Groundwater', fontsize=16)\n",
    "res = seasonal_decompose(df.Depth_to_Groundwater, freq=52, model='additive', extrapolate_trend='freq')\n",
    "\n",
    "res.observed.plot(ax=ax[0, 1], legend=False, color='dodgerblue')\n",
    "ax[0, 1].set_ylabel('Observed', fontsize=14)\n",
    "\n",
    "res.trend.plot(ax=ax[1, 1], legend=False, color='dodgerblue')\n",
    "ax[1, 1].set_ylabel('Trend', fontsize=14)\n",
    "\n",
    "res.seasonal.plot(ax=ax[2, 1], legend=False, color='dodgerblue')\n",
    "ax[2, 1].set_ylabel('Seasonal', fontsize=14)\n",
    "\n",
    "res.resid.plot(ax=ax[3, 1], legend=False, color='dodgerblue')\n",
    "ax[3, 1].set_ylabel('Residual', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "df[['Rainfall', 'Rainfall_trend', 'Rainfall_seasonal', \n",
    "          'Temperature', 'Temperature_trend', 'Temperature_seasonal', \n",
    "          'Drainage_Volume', 'Drainage_Volume_trend', 'Drainage_Volume_seasonal',\n",
    "          'River_Hydrometry', 'River_Hydrometry_trend', 'River_Hydrometry_seasonal', \n",
    "          'Depth_to_Groundwater', 'Depth_to_Groundwater_trend', 'Depth_to_Groundwater_seasonal']].head()\\\n",
    ".style.set_properties(subset=['Rainfall_trend', 'Rainfall_seasonal', \n",
    "                              'Temperature_trend', 'Temperature_seasonal', \n",
    "                              'Drainage_Volume_trend', 'Drainage_Volume_seasonal', \n",
    "                              'River_Hydrometry_trend', 'River_Hydrometry_seasonal',\n",
    "                              'Depth_to_Groundwater_trend', 'Depth_to_Groundwater_seasonal'\n",
    "                             ], **{'background-color': 'dodgerblue'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag\n",
    "`.shift()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_in_month = 4\n",
    "\n",
    "df['Temperature_seasonal_shift_r_2M'] = df.Temperature_seasonal.shift(-2*weeks_in_month)\n",
    "df['Temperature_seasonal_shift_r_1M'] = df.Temperature_seasonal.shift(-1*weeks_in_month)\n",
    "df['Temperature_seasonal_shift_1M'] = df.Temperature_seasonal.shift(1*weeks_in_month)\n",
    "df['Temperature_seasonal_shift_2M'] = df.Temperature_seasonal.shift(2*weeks_in_month)\n",
    "df['Temperature_seasonal_shift_3M'] = df.Temperature_seasonal.shift(3*weeks_in_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "df['Drainage_Volume_seasonal_shift_r_2M'] = df.Drainage_Volume_seasonal.shift(-2*weeks_in_month)\n",
    "df['Drainage_Volume_seasonal_shift_r_1M'] = df.Drainage_Volume_seasonal.shift(-1*weeks_in_month)\n",
    "df['Drainage_Volume_seasonal_shift_1M'] = df.Drainage_Volume_seasonal.shift(1*weeks_in_month)\n",
    "df['Drainage_Volume_seasonal_shift_2M'] = df.Drainage_Volume_seasonal.shift(2*weeks_in_month)\n",
    "df['Drainage_Volume_seasonal_shift_3M'] = df.Drainage_Volume_seasonal.shift(3*weeks_in_month)\n",
    "\n",
    "df['River_Hydrometry_seasonal_shift_r_2M'] = df.River_Hydrometry_seasonal.shift(-2*weeks_in_month)\n",
    "df['River_Hydrometry_seasonal_shift_r_1M'] = df.River_Hydrometry_seasonal.shift(-1*weeks_in_month)\n",
    "df['River_Hydrometry_seasonal_shift_1M'] = df.River_Hydrometry_seasonal.shift(1*weeks_in_month)\n",
    "df['River_Hydrometry_seasonal_shift_2M'] = df.River_Hydrometry_seasonal.shift(2*weeks_in_month)\n",
    "df['River_Hydrometry_seasonal_shift_3M'] = df.River_Hydrometry_seasonal.shift(3*weeks_in_month)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(16,4))\n",
    "sns.lineplot(x=df.Date, y=df.Temperature_seasonal_shift_r_2M, label='shifted by -2 month', ax=ax, color='lightblue')\n",
    "sns.lineplot(x=df.Date, y=df.Temperature_seasonal_shift_r_1M, label='shifted by -1 month', ax=ax, color='skyblue')\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Temperature_seasonal, label='original', ax=ax, color='darkorange')\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Temperature_seasonal_shift_1M, label='shifted by 1 month', ax=ax, color='dodgerblue')\n",
    "sns.lineplot(x=df.Date, y=df.Temperature_seasonal_shift_2M, label='shifted by 2 month', ax=ax, color='blue')\n",
    "sns.lineplot(x=df.Date, y=df.Temperature_seasonal_shift_3M, label='shifted by 3 month', ax=ax, color='navy')\n",
    "\n",
    "ax.set_title('Shifted Time Series', fontsize=16)\n",
    "\n",
    "ax.set_xlim([date(2017, 6, 30), date(2020, 6, 30)])\n",
    "ax.set_ylabel(ylabel='Temperature Bastia Umbra', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Let's begin by plotting the seasonal components of each feature and comparing the minima and maxima. By doing this, we can already gain some insights:\n",
    "* The depth to groundwater reaches its maximum around May/June and its minimum around November/December\n",
    "* The temperature reaches its maxmium around August and its minimum around January\n",
    "* The volume reaches its maximum around June and its minimum around August/September. It takes longer to reach its maximum than to reach its minimum.\n",
    "* The hydrometry reaches its maximum around March and its minimum around September\n",
    "\n",
    "* The volume and hydrometry reach their minimum roughly around the same time\n",
    "* The volume and hydrometry reach their minimum when the temperature reaches its maximum\n",
    "* Temperature lags begind depth to groundwater by around 2 to 3 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=5, ncols=1, figsize=(15, 12))\n",
    "f.suptitle('Seasonal Components of Features', fontsize=16)\n",
    "sns.lineplot(x=df.Date, y=df.Depth_to_Groundwater_seasonal, ax=ax[0], color='dodgerblue', label='P25')\n",
    "ax[0].set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Temperature_seasonal, ax=ax[1], color='dodgerblue', label='Bastia Umbra')\n",
    "ax[1].set_ylabel(ylabel='Temperature', fontsize=14)\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Drainage_Volume_seasonal, ax=ax[2], color='dodgerblue')\n",
    "ax[2].set_ylabel(ylabel='Volume', fontsize=14)\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.River_Hydrometry_seasonal, ax=ax[3], color='dodgerblue')\n",
    "ax[3].set_ylabel(ylabel='Hydrometry', fontsize=14)\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.Rainfall_seasonal, ax=ax[4], color='dodgerblue')\n",
    "ax[4].set_ylabel(ylabel='Rainfall', fontsize=14)\n",
    "\n",
    "for i in range(5):\n",
    "    ax[i].set_xlim([date(2017, 9, 30), date(2020, 6, 30)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the correlation to the target variables increases if we use the time shifted features in comparison to the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
    "\n",
    "original_cols = ['Depth_to_Groundwater_seasonal', \n",
    "                 'Temperature_seasonal',\n",
    "                 'Drainage_Volume_seasonal', 'River_Hydrometry_seasonal']\n",
    "\n",
    "corrmat = df[original_cols].corr()\n",
    "\n",
    "sns.heatmap(corrmat, annot=True, vmin=-1, vmax=1, cmap='coolwarm_r', ax=ax[0])\n",
    "ax[0].set_title('Correlation Matrix of Original Features', fontsize=16)\n",
    "\n",
    "shifted_cols = [ 'Depth_to_Groundwater_seasonal', \n",
    "                'Temperature_seasonal_shift_r_2M',\n",
    "                'Drainage_Volume_seasonal_shift_1M', 'River_Hydrometry_seasonal_shift_3M']\n",
    "corrmat = df[shifted_cols].corr()\n",
    "\n",
    "sns.heatmap(corrmat, annot=True, vmin=-1, vmax=1, cmap='coolwarm_r', ax=ax[1])\n",
    "ax[1].set_title('Correlation Matrix of Shifted Features', fontsize=16)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation Analysis\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    For further details on this topic, see my other notebook: \n",
    "    <a href=\"https://www.kaggle.com/iamleonie/time-series-interpreting-acf-and-pacf\">Time Series: Interpreting ACF and PACF</a>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "This EDA step is especially important when using [ARIMA](#ARIMA). The autocorrelation analysis helps to identify the AR and MA parameters for the [ARIMA](#ARIMA) model.\n",
    "\n",
    "Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF)\n",
    "\n",
    "* **Autocorrelation  Function (ACF)**: Correlation between time series with a lagged version of itself. The autocorrelation function starts a lag 0, which is the correlation of the time series with itself and therefore results in a correlation of 1. -> <font color='blue'>MA parameter is q significant lags</font>\n",
    "* **Partial Autocorrelation Function (PACF)**: Additional correlation explained by each successive lagged term -> <font color='purple'>AR parameter is p significant lags</font>\n",
    "\n",
    "Autocorrelation helps in detecting seasonality.\n",
    "\n",
    "As we can infer from the graph above, the autocorrelation continues to decrease as the lag increases, confirming that there is no linear association between observations separated by larger lags.\n",
    "\n",
    "For the AR process, we expect that the ACF plot will gradually decrease and simultaneously the PACF should have a sharp drop after p significant lags. To define a MA process, we expect the opposite from the ACF and PACF plots, meaning that: the ACF should show a sharp drop after a certain q number of lags while PACF should show a geometric or gradual decreasing trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(df.Depth_to_Groundwater_diff_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some sinusoidal shape in both ACF and PACF functions. This suggests that both AR and MA processes are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 8))\n",
    "\n",
    "plot_acf(df.Depth_to_Groundwater_diff_1,lags=100, ax=ax[0])\n",
    "plot_pacf(df.Depth_to_Groundwater_diff_1,lags=100, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Spectral Analysis\n",
    "to analyse cyclic behavior\n",
    "Frequency domain analysis\n",
    "\n",
    "## Trend estimation and decomposition\n",
    "used for seasonal adjustment\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "For cross validation, you can use the [Time Series Split](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split) library. \n",
    "In [Time Series Forecasting: Building Intuition](https://www.kaggle.com/iamleonie/time-series-forecasting-building-intuition), I go into depth about different types of time series problems and their cross validation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "N_SPLITS = 3\n",
    "\n",
    "X = df.Date\n",
    "y = df.Depth_to_Groundwater\n",
    "\n",
    "folds = TimeSeriesSplit(n_splits=N_SPLITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=N_SPLITS, ncols=2, figsize=(16, 9))\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "    sns.lineplot(x= X_train, y= y_train, ax=ax[i,0], color='dodgerblue', label='train')\n",
    "    sns.lineplot(x= X_train[len(X_train) - len(X_valid):(len(X_train) - len(X_valid) + len(X_valid))], \n",
    "                 y= y_train[len(X_train) - len(X_valid):(len(X_train) - len(X_valid) + len(X_valid))], \n",
    "                 ax=ax[i,1], color='dodgerblue', label='train')\n",
    "\n",
    "    for j in range(2):\n",
    "        sns.lineplot(x= X_valid, y= y_valid, ax=ax[i, j], color='darkorange', label='validation')\n",
    "    ax[i, 0].set_title(f\"Rolling Window with Adjusting Training Size (Split {i+1})\", fontsize=16)\n",
    "    ax[i, 1].set_title(f\"Rolling Window with Constant Training Size (Split {i+1})\", fontsize=16)\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "    ax[i, 0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n",
    "    ax[i, 1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Time series can be either **univariate or multivariate**:\n",
    "* **Univariate** time series only has a single time-dependent variable.\n",
    "* **Multivariate** time series have a multiple time-dependent variable.\n",
    "\n",
    "Our example originally is a multivariate time series because its has multiple features that are all time-dependent. However, by only looking at the target variable `Depth to Groundwater` we can convert it to a univariate time series.\n",
    "\n",
    "We will focus on a **quarterly forecast**. We will use the **Q2 2020 as test data** and the remaining data will be **split by quarter for cross validation**.\n",
    "\n",
    "We will evaluate the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE) of the models. For metrics are better the smaller they are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Univariate Time Series\n",
    "\n",
    "* Stochastic Models\n",
    "    * [Naive Approach](#Naive-Approach)<br>\n",
    "    * [Moving Average](#Moving-Average)<br>\n",
    "    * [Exponential Smoothing](#MExponential-Smoothing)<br>\n",
    "    * [ARIMA](#ARIMA)<br>\n",
    "    * [Prophet](#Prophet)<br>\n",
    "* Deep Learning\n",
    "    * [LSTM](#LSTM)<br>\n",
    "    * [GRU](#GRU)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "df['quarter_idx'] = (df.quarter != df.quarter.shift(1)).cumsum()\n",
    "\n",
    "target = 'Depth_to_Groundwater'\n",
    "features = [feature for feature in df.columns if feature != target]\n",
    "\n",
    "N_SPLITS = 46\n",
    "\n",
    "X = df[df.quarter_idx < N_SPLITS][features]\n",
    "y = df[df.quarter_idx < N_SPLITS][target]\n",
    "\n",
    "X_test = df[df.quarter_idx == N_SPLITS][features].reset_index(drop=True)\n",
    "y_test = df[df.quarter_idx == N_SPLITS][target].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "folds = np.linspace(0, N_SPLITS-3, num=N_SPLITS-2)\n",
    "\n",
    "f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 8))\n",
    "\n",
    "sns.lineplot(x=X.Date, y=y, ax=ax[0], color='dodgerblue', label='train')\n",
    "sns.lineplot(x=X_test.Date, y=y_test, ax=ax[0], color='darkorange', label='test')\n",
    "\n",
    "sns.lineplot(x=df.Date, y=df.quarter_idx, ax=ax[1], color='dodgerblue')\n",
    "ax[0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n",
    "ax[1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n",
    "ax[1].set_ylim([0, N_SPLITS+1])\n",
    "#ax[0].set_ylim([-28, -23])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def plot_approach_evaluation(y_pred, score_mae, score_rsme, approach_name):\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
    "    f.suptitle(approach_name, fontsize=16)\n",
    "    sns.lineplot(x=X.Date, y=y, ax=ax[0], color='dodgerblue', label='Training', linewidth=2)\n",
    "    sns.lineplot(x=X_test.Date, y=y_test, ax=ax[0], color='gold', label='Ground Truth', linewidth=2) #navajowhite\n",
    "    sns.lineplot(x=X_test.Date, y=y_pred, ax=ax[0], color='darkorange', label='Predicted', linewidth=2)\n",
    "    ax[0].set_xlim([date(2018, 6, 30), date(2020, 6, 30)])\n",
    "    ax[0].set_ylim([-27, -23])\n",
    "    ax[0].set_title(f'Prediction \\n MAE: {mean_absolute_error(y_test, y_pred):.2f}, RSME: {math.sqrt(mean_squared_error(y_valid, y_valid_pred)):.2f}', fontsize=14)\n",
    "    ax[0].set_xlabel(xlabel='Date', fontsize=14)\n",
    "    ax[0].set_ylabel(ylabel='Depth to Groundwater P25', fontsize=14)\n",
    "\n",
    "    sns.lineplot(x=folds, y=score_mae,  color='gold', label='MAE', ax=ax[1])#marker='o',\n",
    "    sns.lineplot(x=folds, y=score_rsme, color='indianred', label='RSME', ax=ax[1])\n",
    "    ax[1].set_title('Loss', fontsize=14)\n",
    "    ax[1].set_xlabel(xlabel='Fold', fontsize=14)\n",
    "    ax[1].set_ylabel(ylabel='Loss', fontsize=14)\n",
    "    ax[1].set_ylim([0, 4])   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach\n",
    "\n",
    "$\\hat y_{t+1} = y_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "score_mae = []\n",
    "score_rsme = []\n",
    "for fold, valid_quarter_id in enumerate(range(2, N_SPLITS)):\n",
    "    # Get indices for this fold\n",
    "    train_index = df[df.quarter_idx < valid_quarter_id].index\n",
    "    valid_index = df[df.quarter_idx == valid_quarter_id].index\n",
    "\n",
    "    # Prepare training and validation data for this fold\n",
    "    #X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    # Initialize y_valid_pred\n",
    "    y_valid_pred = pd.Series(np.ones(len(y_valid)))\n",
    "    \n",
    "    # Prediction: Naive approach\n",
    "    y_valid_pred = y_valid_pred * y_train.iloc[-1]\n",
    "    \n",
    "    # Calcuate metrics\n",
    "    score_mae.append(mean_absolute_error(y_valid, y_valid_pred))\n",
    "    score_rsme.append(math.sqrt(mean_squared_error(y_valid, y_valid_pred)))\n",
    "\n",
    "y_pred = pd.Series(np.ones(len(X_test))) * y.iloc[-1]\n",
    "\n",
    "plot_approach_evaluation(y_pred, score_mae, score_rsme, 'Naive Approach')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "score_mae = []\n",
    "score_rsme = []\n",
    "for fold, valid_quarter_id in enumerate(range(2, N_SPLITS)):\n",
    "    # Get indices for this fold\n",
    "    train_index = df[df.quarter_idx < valid_quarter_id].index\n",
    "    valid_index = df[df.quarter_idx == valid_quarter_id].index\n",
    "\n",
    "    # Prepare training and validation data for this fold\n",
    "    #X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    # Initialize y_valid_pred\n",
    "    y_valid_pred = pd.Series(np.ones(len(y_valid)))\n",
    "    \n",
    "    # Prediction: Naive approach    \n",
    "    for i in range(len(y_valid_pred)):\n",
    "        y_valid_pred.iloc[i] = y_train.append(y_valid_pred.iloc[:(i)]).reset_index(drop=True).rolling(4).mean().iloc[-1]\n",
    "        \n",
    "    # Calcuate metrics\n",
    "    score_mae.append(mean_absolute_error(y_valid, y_valid_pred))\n",
    "    score_rsme.append(math.sqrt(mean_squared_error(y_valid, y_valid_pred)))\n",
    "\n",
    "y_pred = pd.Series(np.zeros(len(X_test)))\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    y_pred.iloc[i] = y.append(y_pred.iloc[:(i)]).reset_index(drop=True).rolling(4).mean().iloc[-1]\n",
    "\n",
    "plot_approach_evaluation(y_pred, score_mae, score_rsme, 'Moving Average (Window = 4 Weeks)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither the Naive Approach nor the Moving Average Approach are yielding good results for our example. Usually, these approaches serve as a benchmark rather than the method of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA\n",
    "The Auto-Regressive Integrated Moving Average (ARIMA) model describes the **autocorrelations** in the data. The model assumes that the time-series is **stationary**. It consists of three main parts:\n",
    "* <font color='purple'>Auto-Regressive (AR) filter (long term)</font>: \n",
    "    \n",
    "    $\\color{purple}{y_t = c + \\alpha_1 y_{t-1} + \\dots \\alpha_{\\color{purple}p}y_{t-\\color{purple}p} + \\epsilon_t = c + \\sum_{i=1}^p{\\alpha_i}y_{t-i} + \\epsilon_t}$  -> p\n",
    "* <font color='orange'> Integration filter (stochastic trend)</font>\n",
    "    \n",
    "    -> d\n",
    "* <font color='blue'>Moving Average (MA) filter (short term)</font>:\n",
    "\n",
    "    $\\color{blue}{y_t = c + \\epsilon_t + \\beta_1 \\epsilon_{t-1} + \\dots + \\beta_{q} \\epsilon_{t-q} = c + \\epsilon_t + \\sum_{i=1}^q{\\beta_i}\\epsilon_{t-i}} $  -> q \n",
    "\n",
    "\n",
    "**ARIMA**: $y_t = c + \\color{purple}{\\alpha_1 y_{t-1} + \\dots + \\alpha_{\\color{purple}p}y_{t-\\color{purple}p}} \n",
    "+ \\color{blue}{\\epsilon_t + \\beta_1 \\epsilon_{t-1} + \\dots + \\beta_{q} \\epsilon_{t-q}}$\n",
    "\n",
    "\n",
    "ARIMA(\n",
    "<font color='purple'>p</font>,\n",
    "<font color='orange'>d</font>,\n",
    "<font color='blue'>q</font>)\n",
    "\n",
    "* <font color='purple'>p</font>: Lag order (to determine see  PACF in [Autocorrelation Analysis](#Autocorrelation-Analysis))\n",
    "* <font color='orange'>d</font>: Degree of differencing. (to determine see  Differencing in [Stationarity](#Stationarity))\n",
    "* <font color='blue'>q</font>: Order of moving average (to determine see  ACF in [Autocorrelation Analysis](#Autocorrelation-Analysis))\n",
    "\n",
    "In our example, we can use <font color='orange'>d=0</font> if we use the feature `Depth_to_Groundwater_diff_1`, which is `Depth_to_Groundwater` differenced by the first degree. Otherwise, if we were to use the non-stationary feature `Depth_to_Groundwater` as it is, we should set <font color='orange'>d=1</font>.\n",
    "\n",
    "(work in progress...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "score_mae = []\n",
    "score_rsme = []\n",
    "\n",
    "for fold, valid_quarter_id in enumerate(range(2, N_SPLITS)):\n",
    "    # Get indices for this fold\n",
    "    train_index = df[df.quarter_idx < valid_quarter_id].index\n",
    "    valid_index = df[df.quarter_idx == valid_quarter_id].index\n",
    "\n",
    "    # Prepare training and validation data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    # Fit model with Vector Auto Regression (VAR)\n",
    "    model = ARIMA(y_train, order=(1,1,1))\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # Prediction with Vector Auto Regression (VAR)\n",
    "    y_valid_pred = model_fit.predict(valid_index[0], valid_index[-1])\n",
    "\n",
    "    # Calcuate metrics\n",
    "    score_mae.append(mean_absolute_error(y_valid, y_valid_pred))\n",
    "    score_rsme.append(math.sqrt(mean_squared_error(y_valid, y_valid_pred)))\n",
    "\n",
    "\n",
    "# Fit model with Vector Auto Regression (VAR)\n",
    "model = ARIMA(y, order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Prediction with Vector Auto Regression (VAR)\n",
    "y_pred = model_fit.predict(y.index[-1]+1, y.index[-1] + len(y_test)).reset_index(drop=True)\n",
    "plot_approach_evaluation(y_pred, score_mae, score_rsme, 'ARIMA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Multivariate Time Series\n",
    "\n",
    "### Vector Auto Regression (VAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "score_mae = []\n",
    "score_rsme = []\n",
    "\n",
    "features = ['Temperature', 'Drainage_Volume', 'River_Hydrometry','Rainfall' ]\n",
    "for fold, valid_quarter_id in enumerate(range(2, N_SPLITS)):\n",
    "    # Get indices for this fold\n",
    "    train_index = df[df.quarter_idx < valid_quarter_id].index\n",
    "    valid_index = df[df.quarter_idx == valid_quarter_id].index\n",
    "\n",
    "    # Prepare training and validation data for this fold\n",
    "    X_train, X_valid = X.iloc[train_index][features], X.iloc[valid_index][features]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    # Fit model with Vector Auto Regression (VAR)\n",
    "    model = VAR(pd.concat([y_train, X_train], axis=1))\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # Prediction with Vector Auto Regression (VAR)\n",
    "    y_valid_pred = model_fit.forecast(model_fit.y, steps=len(X_valid))\n",
    "    y_valid_pred = pd.Series(y_valid_pred[:, 0])\n",
    "\n",
    "    # Calcuate metrics\n",
    "    score_mae.append(mean_absolute_error(y_valid, y_valid_pred))\n",
    "    score_rsme.append(math.sqrt(mean_squared_error(y_valid, y_valid_pred)))\n",
    "\n",
    "# Fit model with Vector Auto Regression (VAR)\n",
    "model = VAR(pd.concat([y, X[features]], axis=1))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Prediction with Vector Auto Regression (VAR)\n",
    "y_pred = model_fit.forecast(model_fit.y, steps=len(X_valid))\n",
    "y_pred = pd.Series(y_pred[:, 0])\n",
    "\n",
    "plot_approach_evaluation(y_pred, score_mae, score_rsme, 'Vector Auto Regression (VAR)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Ressources\n",
    "\n",
    "## My Time Series Forecasting Series\n",
    "\n",
    "* [Intro to Time Series Forecasting](https://www.kaggle.com/iamleonie/intro-to-time-series-forecasting)\n",
    "* [Time Series Forecasting: Building Intuition](https://www.kaggle.com/iamleonie/time-series-forecasting-building-intuition)\n",
    "* [Time Series Forecasting: Interpreting ACF and PACF](https://www.kaggle.com/iamleonie/time-series-interpreting-acf-and-pacf)\n",
    "* [Time Series Forecasting: Tips & Tricks for Training LSTMs](https://www.kaggle.com/iamleonie/time-series-tips-tricks-for-training-lstms)\n",
    "\n",
    "## Other Ressources\n",
    "Here are some additional ressources that helped me learn about time series\n",
    "* [Getting started with Time Series using Pandas ](https://www.kaggle.com/parulpandey/getting-started-with-time-series-using-pandas)\n",
    "* [Time Series Analysis || An Introductory Start](https://www.kaggle.com/janiobachmann/time-series-analysis-an-introductory-start)\n",
    "* [Everything you can do with a time series](https://www.kaggle.com/thebrownviking20/everything-you-can-do-with-a-time-series)\n",
    "* [Deep Learning for Time Series | Dimitry Larko | Kaggle Days](https://www.youtube.com/watch?v=svNwWSgz2NM)\n",
    "* [Encoding Cyclical Features for Deep Learning](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n",
    "* [Tamara Louie: Applying Statistical Modeling & Machine Learning to Perform Time-Series Forecasting](https://www.youtube.com/watch?v=JntA9XaTebs)\n",
    "* [Forecasting: Principles and Practice](https://otexts.com/fpp2/)\n",
    "* [Easy Guide on Time Series Forecasting](https://beingdatum.com/time-series-forecasting/)\n",
    "\n",
    "# Useful Libraries\n",
    "* [statsmodels](https://www.statsmodels.org)\n",
    "* [Pandas Time series / date functionality](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html)\n",
    "* [Time Series Split](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Exponential Smoothing\n",
    "based on a description of the **trend and seasonality** in the data\n",
    "\n",
    "### Prophet \n",
    "\n",
    "### LSTM\n",
    "\n",
    "### GRU\n",
    "\n",
    "(work in progress)\n",
    "\n",
    "Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)\n",
    "\n",
    "Recurrent Neural Network (RNN)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
