{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3975a3a8-8d1d-458c-9c18-7deeccbbf205",
    "_execution_state": "idle",
    "_uuid": "05bef3a5fb05cc62c34cace91131b01c452cff01"
   },
   "source": [
    "Find the original notebook at kaggle: https://www.kaggle.com/code/victorambonati/unsupervised-anomaly-detection/data\n",
    "\n",
    "# Motivation : \n",
    "I read an interesting article about anomaly detection: https://iwringer.wordpress.com/2015/11/17/anomaly-detection-concepts-and-techniques/.  \n",
    "I wanted to try a few of these techniques to better understand them. I searched an interesting dataset on Kaggle about anomaly detection with simple exemples. I choose one exemple of NAB datasets (thanks for this datasets) and I implemented a few of these algorithms. The goal of this Notebook is just to implement these techniques and understand there main caracteristics. Sometimes, they are not adapted to this datasets. I add some visualizations to understand what the algorithm detect. Hope it can help some people.\n",
    "Notebook available (with Markov Chain) here: https://github.com/Vicam/Unsupervised_Anomaly_Detection\n",
    "\n",
    "# Algorithm implemented :\n",
    "- Cluster based anomaly detection (K-mean)\n",
    "- Repartition of data into categories then Gaussian/Elliptic Enveloppe on each categories separately\n",
    "- Markov Chain\n",
    "- Isolation Forest\n",
    "- One class SVM\n",
    "- RNN (comparison between prediction and reality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5ac34a72-3992-448a-8ced-12bb329c40a7",
    "_execution_state": "idle",
    "_uuid": "1fd6aebed8495fb904ca1465245627cef301294d",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "#%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import matplotlib.dates as md\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "#from pyemma import msm # not available on Kaggle Kernel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "828ca1cd-759b-4d8a-a592-eaf2ca691780",
    "_execution_state": "idle",
    "_uuid": "9eb06b06b146c9aa9533c1a78203dd1025d6e27e",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# some function for later\n",
    "\n",
    "# return Series of distance between each point and his distance with the closest centroid\n",
    "def getDistanceByPoint(data, model):\n",
    "    distance = pd.Series()\n",
    "    for i in range(0,len(data)):\n",
    "        Xa = np.array(data.loc[i])\n",
    "        Xb = model.cluster_centers_[model.labels_[i]-1]\n",
    "        distance.set_value(i, np.linalg.norm(Xa-Xb))\n",
    "    return distance\n",
    "\n",
    "# train markov model to get transition matrix\n",
    "def getTransitionMatrix (df):\n",
    "\tdf = np.array(df)\n",
    "\tmodel = msm.estimate_markov_model(df, 1)\n",
    "\treturn model.transition_matrix\n",
    "\n",
    "def markovAnomaly(df, windows_size, threshold):\n",
    "    transition_matrix = getTransitionMatrix(df)\n",
    "    real_threshold = threshold**windows_size\n",
    "    df_anomaly = []\n",
    "    for j in range(0, len(df)):\n",
    "        if (j < windows_size):\n",
    "            df_anomaly.append(0)\n",
    "        else:\n",
    "            sequence = df[j-windows_size:j]\n",
    "            sequence = sequence.reset_index(drop=True)\n",
    "            df_anomaly.append(anomalyElement(sequence, real_threshold, transition_matrix))\n",
    "    return df_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3da6fa0d-ef0f-42d1-bd59-ee4fb3cfc2ab",
    "_execution_state": "idle",
    "_uuid": "7459b17ef14b51410807f57f36e887682f2cad33"
   },
   "source": [
    "# 1 Data\n",
    "## 1.1 Extract data\n",
    "The dataset is from https://www.kaggle.com/boltzmannbrain/nab \n",
    "In realKnownCause/ambient_temperature_system_failure.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "bcb68e2d-981d-403d-bfed-655e8a9bdc3c",
    "_execution_state": "idle",
    "_uuid": "e7d412858498c0da43c102f0ef1db787f1652374",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../input/realKnownCause/realKnownCause/ambient_temperature_system_failure.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/realKnownCause/realKnownCause/ambient_temperature_system_failure.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fc3bc10f-54df-44e2-b3a2-cc8b59717c01",
    "_execution_state": "idle",
    "_uuid": "44c55c1820894f40f650cba31a4723591315345d"
   },
   "source": [
    "## 1.2 Understand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6ce4bfe8-1d91-4172-830e-a760e1d63b1e",
    "_execution_state": "idle",
    "_uuid": "3927fcc620162f8a69100525f83d609cd7b589f4"
   },
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5b77b21-3793-42ab-b291-11189be9071f",
    "_execution_state": "idle",
    "_uuid": "5ef1df3d1671939759548a4199fd05c17ec251e1"
   },
   "outputs": [],
   "source": [
    "# check the timestamp format and frequence \n",
    "print(df['timestamp'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5b0e5bea-f11b-4f81-93fd-a817a6f9f579",
    "_execution_state": "idle",
    "_uuid": "899ad93b122b2144c304ff31f93568e183c32c6c"
   },
   "outputs": [],
   "source": [
    "# check the temperature mean\n",
    "print(df['value'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0395bd90-61f4-49db-b157-b58853afe2f8",
    "_execution_state": "idle",
    "_uuid": "d5e63d3da28366ddf2fbaa2653d75536332713dc"
   },
   "outputs": [],
   "source": [
    "# change the type of timestamp column for plotting\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "# change fahrenheit to Â°C (temperature mean= 71 -> fahrenheit)\n",
    "df['value'] = (df['value'] - 32) * 5/9\n",
    "# plot the data\n",
    "df.plot(x='timestamp', y='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "90075ebb-0f2e-4258-8901-03a25ce67172",
    "_execution_state": "idle",
    "_uuid": "c62e4d5f8bf8a855aab7bfa86273d731a5638b6d"
   },
   "source": [
    "## 1.3 Feature engineering\n",
    "Extracting some useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e1d36c51-0d96-43c5-8a6c-9f26d7997788",
    "_execution_state": "idle",
    "_uuid": "a86baa1be987a4de8b4b192af475ab4fde077d82",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# the hours and if it's night or day (7:00-22:00)\n",
    "df['hours'] = df['timestamp'].dt.hour\n",
    "df['daylight'] = ((df['hours'] >= 7) & (df['hours'] <= 22)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b5dc68e4-89b0-4248-853d-e00e5e9ef4e9",
    "_execution_state": "idle",
    "_uuid": "c3d4a71542f5080b5f87a27cf681f542a665a34c",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# the day of the week (Monday=0, Sunday=6) and if it's a week end day or week day.\n",
    "df['DayOfTheWeek'] = df['timestamp'].dt.dayofweek\n",
    "df['WeekDay'] = (df['DayOfTheWeek'] < 5).astype(int)\n",
    "# An estimation of anomly population of the dataset (necessary for several algorithm)\n",
    "outliers_fraction = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "14cd3d26-a411-47f8-9238-2794284e8c9b",
    "_execution_state": "idle",
    "_uuid": "189cd0890e09373ed847b258d1354bd6eae7bfd8",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# time with int to plot easily\n",
    "df['time_epoch'] = (df['timestamp'].astype(np.int64)/100000000000).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0c5f5102-2122-41da-980d-c58c8438f190",
    "_execution_state": "idle",
    "_uuid": "60e1a0334d22e2c0c96ac279cc48c66f99b43248"
   },
   "outputs": [],
   "source": [
    "# creation of 4 distinct categories that seem useful (week end/day week & night/day)\n",
    "df['categories'] = df['WeekDay']*2 + df['daylight']\n",
    "\n",
    "a = df.loc[df['categories'] == 0, 'value']\n",
    "b = df.loc[df['categories'] == 1, 'value']\n",
    "c = df.loc[df['categories'] == 2, 'value']\n",
    "d = df.loc[df['categories'] == 3, 'value']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "a_heights, a_bins = np.histogram(a)\n",
    "b_heights, b_bins = np.histogram(b, bins=a_bins)\n",
    "c_heights, c_bins = np.histogram(c, bins=a_bins)\n",
    "d_heights, d_bins = np.histogram(d, bins=a_bins)\n",
    "\n",
    "width = (a_bins[1] - a_bins[0])/6\n",
    "\n",
    "ax.bar(a_bins[:-1], a_heights*100/a.count(), width=width, facecolor='blue', label='WeekEndNight')\n",
    "ax.bar(b_bins[:-1]+width, (b_heights*100/b.count()), width=width, facecolor='green', label ='WeekEndLight')\n",
    "ax.bar(c_bins[:-1]+width*2, (c_heights*100/c.count()), width=width, facecolor='red', label ='WeekDayNight')\n",
    "ax.bar(d_bins[:-1]+width*3, (d_heights*100/d.count()), width=width, facecolor='black', label ='WeekDayLight')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "57fc7caa-fb96-491a-9857-3c6e58e8cade",
    "_execution_state": "idle",
    "_uuid": "3d980e3fcede78e8b9a3719cf3ad1da9ece24bff"
   },
   "source": [
    "We can see that the temperature is more stable during daylight of business day.\n",
    "# 2 Models\n",
    "## 2.1 Cluster only\n",
    "#### Use for collective anomalies (unordered). \n",
    "\n",
    "We group together the usual combination of features. The points that are far from the cluster are points with usual combination of features.We consider those points as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "01224c89-4372-4f8b-bfb8-d13eb4357264",
    "_execution_state": "idle",
    "_uuid": "7dfbf8b613b235120c39e11a8a87f3c5e90022e3",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Take useful feature and standardize them\n",
    "data = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)\n",
    "# reduce to 2 importants features\n",
    "pca = PCA(n_components=2)\n",
    "data = pca.fit_transform(data)\n",
    "# standardize these 2 new features\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "be39cb8b-bcc7-4288-95cb-8c95291a55ba",
    "_execution_state": "idle",
    "_uuid": "452855f2761685d48a0bf360f253ecd18fe379fa"
   },
   "outputs": [],
   "source": [
    "# calculate with different number of centroids to see the loss plot (elbow method)\n",
    "n_cluster = range(1, 20)\n",
    "kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
    "scores = [kmeans[i].score(data) for i in range(len(kmeans))]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_cluster, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "69beec11-d424-4646-b964-d5fbbf8a6550",
    "_execution_state": "idle",
    "_uuid": "0736ab2c718cbc4138b07deef6c20a690dcecdc7"
   },
   "outputs": [],
   "source": [
    "# Not clear for me, I choose 15 centroids arbitrarily and add these data to the central dataframe\n",
    "df['cluster'] = kmeans[14].predict(data)\n",
    "df['principal_feature1'] = data[0]\n",
    "df['principal_feature2'] = data[1]\n",
    "df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d14ac727-eac4-4ae1-99c8-b9fb9b1301e7",
    "_execution_state": "idle",
    "_uuid": "64ac4322fbf2bc36539e0a60ff8e45254dc63afe"
   },
   "outputs": [],
   "source": [
    "#plot the different clusters with the 2 main features\n",
    "fig, ax = plt.subplots()\n",
    "colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black', 5:'orange', 6:'cyan', 7:'yellow', 8:'brown', 9:'purple', 10:'white', 11: 'grey', 12:'lightblue', 13:'lightgreen', 14: 'darkgrey'}\n",
    "ax.scatter(df['principal_feature1'], df['principal_feature2'], c=df[\"cluster\"].apply(lambda x: colors[x]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cd260641-7e32-4305-b7a8-008fcff4dcfc",
    "_execution_state": "idle",
    "_uuid": "1a84160cfb1d419c3a0a57530c079bfa6b1a11b1",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly\n",
    "distance = getDistanceByPoint(data, kmeans[14])\n",
    "number_of_outliers = int(outliers_fraction*len(distance))\n",
    "threshold = distance.nlargest(number_of_outliers).min()\n",
    "# anomaly21 contain the anomaly result of method 2.1 Cluster (0:normal, 1:anomaly) \n",
    "df['anomaly21'] = (distance >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "38a6fd23-954f-400c-8065-f4bea22183d5",
    "_execution_state": "idle",
    "_uuid": "40e05059a9a66b52fba53c654232a356df3a8828"
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly with cluster view\n",
    "fig, ax = plt.subplots()\n",
    "colors = {0:'blue', 1:'red'}\n",
    "ax.scatter(df['principal_feature1'], df['principal_feature2'], c=df[\"anomaly21\"].apply(lambda x: colors[x]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "56a2455c-ee60-491e-8010-aab56eeb8e3f",
    "_execution_state": "idle",
    "_uuid": "8ae64bd8c8b942e2b8aaf9e6b75d40df7b5c4c6c"
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly throughout time (viz 1)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "a = df.loc[df['anomaly21'] == 1, ['time_epoch', 'value']] #anomaly\n",
    "\n",
    "ax.plot(df['time_epoch'], df['value'], color='blue')\n",
    "ax.scatter(a['time_epoch'],a['value'], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9926f2fb-6fa2-44e8-8b97-c5e4b6de311f",
    "_execution_state": "idle",
    "_uuid": "3b12e1fe4c788500c1734de1c676dd4f429afe7c"
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly with temperature repartition (viz 2)\n",
    "a = df.loc[df['anomaly21'] == 0, 'value']\n",
    "b = df.loc[df['anomaly21'] == 1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "211bffc8-1a4e-4e47-87af-2c50dd896d02",
    "_execution_state": "idle",
    "_uuid": "c7840c929e610cd29d1775e30ad45364407dc7a0"
   },
   "source": [
    "Cluster method detects the low temperature around the end of record as unusually low. It doesn't detect the highest temperature pic.\n",
    "## 2.2 Categories + Gaussian\n",
    "#### Use for contextual data and collective anomalies (unordered).  \n",
    "We will separate data by (what we think of) important categories. Or we can separate data based on different cluster (method 2.3). Then we find outliers (gaussian repartition, unimodal) by categories independently.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "46755793-473d-413d-a0fc-b08b7d589676",
    "_execution_state": "idle",
    "_uuid": "0c35172a17989a3cf536fca25a4560a933eaa374",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# creation of 4 differents data set based on categories defined before\n",
    "df_class0 = df.loc[df['categories'] == 0, 'value']\n",
    "df_class1 = df.loc[df['categories'] == 1, 'value']\n",
    "df_class2 = df.loc[df['categories'] == 2, 'value']\n",
    "df_class3 = df.loc[df['categories'] == 3, 'value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1b973d0e-300a-4875-834d-d6011b43d25f",
    "_execution_state": "idle",
    "_uuid": "8562a05945620307d0a63ccd36da46e5c4434a3c"
   },
   "outputs": [],
   "source": [
    "# plot the temperature repartition by categories\n",
    "fig, axs = plt.subplots(2,2)\n",
    "df_class0.hist(ax=axs[0,0],bins=32)\n",
    "df_class1.hist(ax=axs[0,1],bins=32)\n",
    "df_class2.hist(ax=axs[1,0],bins=32)\n",
    "df_class3.hist(ax=axs[1,1],bins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f269fc3e-92da-470a-89ba-83cc51c07272",
    "_execution_state": "idle",
    "_uuid": "bbedf1d3f83cf7ae6b4ff65f979479a44b03bfe0",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# apply ellipticEnvelope(gaussian distribution) at each categories\n",
    "envelope =  EllipticEnvelope(contamination = outliers_fraction) \n",
    "X_train = df_class0.values.reshape(-1,1)\n",
    "envelope.fit(X_train)\n",
    "df_class0 = pd.DataFrame(df_class0)\n",
    "df_class0['deviation'] = envelope.decision_function(X_train)\n",
    "df_class0['anomaly'] = envelope.predict(X_train)\n",
    "\n",
    "envelope =  EllipticEnvelope(contamination = outliers_fraction) \n",
    "X_train = df_class1.values.reshape(-1,1)\n",
    "envelope.fit(X_train)\n",
    "df_class1 = pd.DataFrame(df_class1)\n",
    "df_class1['deviation'] = envelope.decision_function(X_train)\n",
    "df_class1['anomaly'] = envelope.predict(X_train)\n",
    "\n",
    "envelope =  EllipticEnvelope(contamination = outliers_fraction) \n",
    "X_train = df_class2.values.reshape(-1,1)\n",
    "envelope.fit(X_train)\n",
    "df_class2 = pd.DataFrame(df_class2)\n",
    "df_class2['deviation'] = envelope.decision_function(X_train)\n",
    "df_class2['anomaly'] = envelope.predict(X_train)\n",
    "\n",
    "envelope =  EllipticEnvelope(contamination = outliers_fraction) \n",
    "X_train = df_class3.values.reshape(-1,1)\n",
    "envelope.fit(X_train)\n",
    "df_class3 = pd.DataFrame(df_class3)\n",
    "df_class3['deviation'] = envelope.decision_function(X_train)\n",
    "df_class3['anomaly'] = envelope.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b309d1b4-3bfe-4803-85c0-f03b71bb00fc",
    "_execution_state": "idle",
    "_uuid": "f622f36a8c6d0d1abfde310cf25a0ff5ddba1c78"
   },
   "outputs": [],
   "source": [
    "# plot the temperature repartition by categories with anomalies\n",
    "a0 = df_class0.loc[df_class0['anomaly'] == 1, 'value']\n",
    "b0 = df_class0.loc[df_class0['anomaly'] == -1, 'value']\n",
    "\n",
    "a1 = df_class1.loc[df_class1['anomaly'] == 1, 'value']\n",
    "b1 = df_class1.loc[df_class1['anomaly'] == -1, 'value']\n",
    "\n",
    "a2 = df_class2.loc[df_class2['anomaly'] == 1, 'value']\n",
    "b2 = df_class2.loc[df_class2['anomaly'] == -1, 'value']\n",
    "\n",
    "a3 = df_class3.loc[df_class3['anomaly'] == 1, 'value']\n",
    "b3 = df_class3.loc[df_class3['anomaly'] == -1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots(2,2)\n",
    "axs[0,0].hist([a0,b0], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "axs[0,1].hist([a1,b1], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "axs[1,0].hist([a2,b2], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "axs[1,1].hist([a3,b3], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "axs[0,0].set_title(\"WeekEndNight\")\n",
    "axs[0,1].set_title(\"WeekEndLight\")\n",
    "axs[1,0].set_title(\"WeekDayNight\")\n",
    "axs[1,1].set_title(\"WeekDayLight\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "771633b5-7924-4f98-9a9e-66dff0d8fb1e",
    "_execution_state": "idle",
    "_uuid": "250cab346e794bbdf6b3860e8880bb2c13d5a270",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# add the data to the main \n",
    "df_class = pd.concat([df_class0, df_class1, df_class2, df_class3])\n",
    "df['anomaly22'] = df_class['anomaly']\n",
    "df['anomaly22'] = np.array(df['anomaly22'] == -1).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ea6efd5d-014d-4257-9522-5f2560f9a7df",
    "_execution_state": "idle",
    "_uuid": "533cd11f31bc2ad2b2381b38a3ee722566ddb195"
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly throughout time (viz 1)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "a = df.loc[df['anomaly22'] == 1, ('time_epoch', 'value')] #anomaly\n",
    "\n",
    "ax.plot(df['time_epoch'], df['value'], color='blue')\n",
    "ax.scatter(a['time_epoch'],a['value'], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9bdd7add-99b4-42c6-9d3f-3670b9f9ebcf",
    "_execution_state": "idle",
    "_uuid": "0642b30ee151c600f5d9141a1b36b69cbd328a2c"
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly with temperature repartition (viz 2)\n",
    "a = df.loc[df['anomaly22'] == 0, 'value']\n",
    "b = df.loc[df['anomaly22'] == 1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "55a0778e-3606-4ce1-847c-06c148c55fb5",
    "_execution_state": "idle",
    "_uuid": "033a39b8f36b5089417199aeb21e37242cff0403"
   },
   "source": [
    "Good detections of extreme values and context separation add some precision to the detection.\n",
    "## 2.3 Cluster+Gaussian\n",
    "Similar to 2.2 solution but with cluster to separate data in different group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e966a2b5-b49d-489b-bb8f-88eff97deb6d",
    "_execution_state": "idle",
    "_uuid": "f796a11f0beacf7b60b9c02118f4517a0597cbff"
   },
   "source": [
    "## 2.4 Markov chains\n",
    "#### Use for  sequential anomalies (ordered)\n",
    "We need discretize the data points in define states for markov chain. We will just take 'value' to define state for this example and define 5 levels of value (very low, low, average, high, very high)/(VL, L, A, H, VH).\n",
    "Markov chain will calculate the probability of sequence like (VL, L, L, A, A, L, A). If the probability is very weak we consider the sequence as an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "02d2db70-197e-4fc2-b027-b20f8565c463",
    "_execution_state": "idle",
    "_uuid": "f54f23f37f06672cb19d89765a6cbfadcbe32369",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# definition of the different state\n",
    "x1 = (df['value'] <=18).astype(int)\n",
    "x2= ((df['value'] > 18) & (df['value']<=21)).astype(int)\n",
    "x3 = ((df['value'] > 21) & (df['value']<=24)).astype(int)\n",
    "x4 = ((df['value'] > 24) & (df['value']<=27)).astype(int)\n",
    "x5 = (df['value'] >27).astype(int)\n",
    "df_mm = x1 + 2*x2 + 3*x3 + 4*x4 + 5*x5\n",
    "\n",
    "# getting the anomaly labels for our dataset (evaluating sequence of 5 values and anomaly = less than 20% probable)\n",
    "# I USE pyemma NOT AVAILABLE IN KAGGLE KERNEL\n",
    "#df_anomaly = markovAnomaly(df_mm, 5, 0.20)\n",
    "#df_anomaly = pd.Series(df_anomaly)\n",
    "#print(df_anomaly.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "20f432e1-f393-4bc3-8d2d-32e4397e58a1",
    "_execution_state": "idle",
    "_uuid": "972ce803b0a56160d465e6c1f6ff0ddb9d564d85",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# add the data to the main \n",
    "df['anomaly24'] = df_anomaly\n",
    "\n",
    "# visualisation of anomaly throughout time (viz 1)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "a = df.loc[df['anomaly24'] == 1, ('time_epoch', 'value')] #anomaly\n",
    "\n",
    "ax.plot(df['time_epoch'], df['value'], color='blue')\n",
    "ax.scatter(a['time_epoch'],a['value'], color='red')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c92f059a-7938-43f7-8d2d-426f772bae84",
    "_execution_state": "idle",
    "_uuid": "a73dbcfb3d0cddbde36fbd1dbc5a2a45e69f1262",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# visualisation of anomaly with temperature repartition (viz 2)\n",
    "a = df.loc[df['anomaly24'] == 0, 'value']\n",
    "b = df.loc[df['anomaly24'] == 1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "796a1096-dd90-4bfd-ba52-1fc3c0b99a16",
    "_execution_state": "idle",
    "_uuid": "2e6accac972f6adf0d907f458943ef5f2129e7df"
   },
   "source": [
    "Detect unusual sequence but not extreme value. More difficult to evaluate the relevance on this example. The sequence size (5) should be match with some interesting cycle.\n",
    "## 2.5 Isolation Forest\n",
    "#### Use for collective anomalies (unordered).\n",
    "Simple, works well with different data repartition and efficient with high dimention data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7b550505-0752-43a0-b21d-6dc0fd8105e2",
    "_execution_state": "idle",
    "_uuid": "0a5641fd4d56e1985f923a92f40138676a4fe54f"
   },
   "outputs": [],
   "source": [
    "# Take useful feature and standardize them \n",
    "data = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)\n",
    "# train isolation forest \n",
    "model =  IsolationForest(contamination = outliers_fraction)\n",
    "model.fit(data)\n",
    "# add the data to the main  \n",
    "df['anomaly25'] = pd.Series(model.predict(data))\n",
    "df['anomaly25'] = df['anomaly25'].map( {1: 0, -1: 1} )\n",
    "print(df['anomaly25'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "34e8c98b-a99d-44f7-8aad-a599371b3703",
    "_execution_state": "idle",
    "_uuid": "f5ac7ddd6a6ac149c6e88664abc9fb5401756261"
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly throughout time (viz 1)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "a = df.loc[df['anomaly25'] == 1, ['time_epoch', 'value']] #anomaly\n",
    "\n",
    "ax.plot(df['time_epoch'], df['value'], color='blue')\n",
    "ax.scatter(a['time_epoch'],a['value'], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "62a32613-6f21-46db-9212-0286be3d15dc",
    "_execution_state": "idle",
    "_uuid": "d4b909d1a62d7a5e2dd9f235d4fa31eb8d5a148f"
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly with temperature repartition (viz 2)\n",
    "a = df.loc[df['anomaly25'] == 0, 'value']\n",
    "b = df.loc[df['anomaly25'] == 1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label = ['normal', 'anomaly'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2c56928d-e7fa-4342-8bc5-23527ae69747",
    "_execution_state": "idle",
    "_uuid": "9c616d8efd272ea7aeddee0d8bb2a41d803209cd"
   },
   "source": [
    "## 2.6 One class SVM\n",
    "#### Use for collective anomalies (unordered).\n",
    "Good for novelty detection (no anomalies in the train set). This algorithm performs well for multimodal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "16aea1c0-9a4c-489a-8b0c-b9e1893fba75",
    "_execution_state": "idle",
    "_uuid": "31b6408bed29fc271174502e699f12a3dd7deb9e"
   },
   "outputs": [],
   "source": [
    "# Take useful feature and standardize them \n",
    "data = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data)\n",
    "# train one class SVM \n",
    "model =  OneClassSVM(nu=0.95 * outliers_fraction) #nu=0.95 * outliers_fraction  + 0.05\n",
    "data = pd.DataFrame(np_scaled)\n",
    "model.fit(data)\n",
    "# add the data to the main  \n",
    "df['anomaly26'] = pd.Series(model.predict(data))\n",
    "df['anomaly26'] = df['anomaly26'].map( {1: 0, -1: 1} )\n",
    "print(df['anomaly26'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c1ba12cb-4035-4082-9097-f9ebe4d5a550",
    "_execution_state": "idle",
    "_uuid": "a9cd247e4b1a081f538fe18305c98ed3e67429b3"
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly throughout time (viz 1)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "a = df.loc[df['anomaly26'] == 1, ['time_epoch', 'value']] #anomaly\n",
    "\n",
    "ax.plot(df['time_epoch'], df['value'], color='blue')\n",
    "ax.scatter(a['time_epoch'],a['value'], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "71e58d95-e373-40b4-b7a7-f153b994fdc5",
    "_execution_state": "idle",
    "_uuid": "5fd47274d72716015023ef9ea94ae54befa056b7"
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly with temperature repartition (viz 2)\n",
    "a = df.loc[df['anomaly26'] == 0, 'value']\n",
    "b = df.loc[df['anomaly26'] == 1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b941a3a-1b7c-402a-9b3c-3de9cf50669b",
    "_execution_state": "idle",
    "_uuid": "379754b0742140516dde2963c36ea7db00139035"
   },
   "source": [
    "Give result similar to isolation forest but find some anomalies in average values. Difficult to know if it's relevant.\n",
    "## 2.7 RNN\n",
    "#### Use for  sequential anomalies (ordered)\n",
    "RNN learn to recognize sequence in the data and then make prediction based on the previous sequence. We consider an anomaly when the next data points are distant from RNN prediction. Aggregation, size of sequence and size of prediction for anomaly are important parameters to have relevant detection.  \n",
    "Here we make learn from 50 previous values, and we predict just the 1 next value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dd04af3e-0b01-435a-b4fc-92666de95289",
    "_execution_state": "idle",
    "_uuid": "2fef8ac3eaa6116697b067f74be61a725898d784",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#select and standardize data\n",
    "data_n = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data_n)\n",
    "data_n = pd.DataFrame(np_scaled)\n",
    "\n",
    "# important parameters and train/test size\n",
    "prediction_time = 1 \n",
    "testdatasize = 1000\n",
    "unroll_length = 50\n",
    "testdatacut = testdatasize + unroll_length  + 1\n",
    "\n",
    "#train data\n",
    "x_train = data_n[0:-prediction_time-testdatacut].as_matrix()\n",
    "y_train = data_n[prediction_time:-testdatacut  ][0].as_matrix()\n",
    "\n",
    "# test data\n",
    "x_test = data_n[0-testdatacut:-prediction_time].as_matrix()\n",
    "y_test = data_n[prediction_time-testdatacut:  ][0].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9ccb1c44-a714-45bd-8313-d6fc941bf220",
    "_execution_state": "idle",
    "_uuid": "be6f2d72557f6cb87179157efca44cdeab96b057"
   },
   "outputs": [],
   "source": [
    "#unroll: create sequence of 50 previous data points for each data points\n",
    "def unroll(data,sequence_length=24):\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "    return np.asarray(result)\n",
    "\n",
    "# adapt the datasets for the sequence data shape\n",
    "x_train = unroll(x_train,unroll_length)\n",
    "x_test  = unroll(x_test,unroll_length)\n",
    "y_train = y_train[-x_train.shape[0]:]\n",
    "y_test  = y_test[-x_test.shape[0]:]\n",
    "\n",
    "# see the shape\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9089b29d-a698-407d-8675-089a8641e153",
    "_execution_state": "idle",
    "_uuid": "d2d991c7cb9ab07c73b329e9cb7378d5dd2a3212"
   },
   "outputs": [],
   "source": [
    "# specific libraries for RNN\n",
    "# keras is a high layer build on Tensorflow layer to stay in high level/easy implementation\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "import time #helper libraries\n",
    "from keras.models import model_from_json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "33166362-a1a8-4adb-a6fc-afb10cb8b1b4",
    "_execution_state": "idle",
    "_uuid": "bd87547d94e2d128a755c080e39386dfe642e8b8"
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "    input_dim=x_train.shape[-1],\n",
    "    output_dim=50,\n",
    "    return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(\n",
    "    100,\n",
    "    return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(\n",
    "    units=1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "start = time.time()\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "print('compilation time : {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8e70774e-68d7-46ed-8434-edefc1db397f",
    "_execution_state": "busy",
    "_uuid": "5adb8e2b1b7d1f7d541b6b306b8d98885b37500a"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "#nb_epoch = 350\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=3028,\n",
    "    nb_epoch=30,\n",
    "    validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2f77488f-2a11-4abf-9d5d-28106b9b7543",
    "_execution_state": "idle",
    "_uuid": "53c665cfadb466197cdbd5c4390e37fd8bf9084e"
   },
   "outputs": [],
   "source": [
    "# save the model because the training is long (1h30) and we don't want to do it every time\n",
    "\"\"\"\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model2.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7f4b7842-5081-4f20-a082-d0f971949c34",
    "_execution_state": "idle",
    "_uuid": "db2f64492cc4929317153ecb4d0140f2f28c1e90"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "\"\"\"\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "adb69c0d-ff41-49c5-bf30-335f85ef0ec5",
    "_execution_state": "idle",
    "_uuid": "f65429b7f2aed953e7db2c3276e4fcdd764df476",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create the list of difference between prediction and test data\n",
    "loaded_model = model\n",
    "diff=[]\n",
    "ratio=[]\n",
    "p = loaded_model.predict(x_test)\n",
    "# predictions = lstm.predict_sequences_multiple(loaded_model, x_test, 50, 50)\n",
    "for u in range(len(y_test)):\n",
    "    pr = p[u][0]\n",
    "    ratio.append((y_test[u]/pr)-1)\n",
    "    diff.append(abs(y_test[u]- pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "35249507-6673-4ac0-9196-b75f1cd5c395",
    "_execution_state": "idle",
    "_uuid": "2a0eb8a0ad91203f2797eb0602d6143bec46b96e"
   },
   "outputs": [],
   "source": [
    "# plot the prediction and the reality (for the test data)\n",
    "fig, axs = plt.subplots()\n",
    "axs.plot(p,color='red', label='prediction')\n",
    "axs.plot(y_test,color='blue', label='y_test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7011b0e3-e9b6-4ea0-bb0e-1c44b11ad950",
    "_execution_state": "idle",
    "_uuid": "0b6f73d8f09bdd06ced35203de4250807084122d"
   },
   "outputs": [],
   "source": [
    "# select the most distant prediction/reality data points as anomalies\n",
    "diff = pd.Series(diff)\n",
    "number_of_outliers = int(outliers_fraction*len(diff))\n",
    "threshold = diff.nlargest(number_of_outliers).min()\n",
    "# data with anomaly label (test data part)\n",
    "test = (diff >= threshold).astype(int)\n",
    "# the training data part where we didn't predict anything (overfitting possible): no anomaly\n",
    "complement = pd.Series(0, index=np.arange(len(data_n)-testdatasize))\n",
    "# # add the data to the main\n",
    "df['anomaly27'] = complement.append(test, ignore_index='True')\n",
    "print(df['anomaly27'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c5230ec9-70cc-48de-aab0-b1f79c442a60",
    "_execution_state": "idle",
    "_uuid": "eea00d2cae4842b1ac445a8252844fb291905b4f"
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly throughout time (viz 1)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "a = df.loc[df['anomaly27'] == 1, ['time_epoch', 'value']] #anomaly\n",
    "\n",
    "ax.plot(df['time_epoch'], df['value'], color='blue')\n",
    "ax.scatter(a['time_epoch'],a['value'], color='red')\n",
    "plt.axis([1.370*1e7, 1.405*1e7, 15,30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7ab6f44f-f316-406c-b72b-069fb1aa162f",
    "_execution_state": "idle",
    "_uuid": "4400b2bf187ba69b7616f19eee23bc2277c2d766"
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly with temperature repartition (viz 2)\n",
    "a = df.loc[df['anomaly27'] == 0, 'value']\n",
    "b = df.loc[df['anomaly27'] == 1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "43e8a0e9-63eb-462c-a08b-996cb27dfa5f",
    "_execution_state": "idle",
    "_uuid": "6a419593698fe1d6ef85deec4401b1767e7d4538"
   },
   "source": [
    "## 2.8 Collective and sequential anomalies (Ordered)\n",
    "This class is most general and consider ordering as well as value combinations. We usually use combination of algorithm like cluster+markov model.\n",
    "## 3 Result comparison\n",
    "(may be later)\n",
    "## 4 Conclusion\n",
    "For this case, the contextual anomaly detection (categories+elliptique enveloppe) seem a good solution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
